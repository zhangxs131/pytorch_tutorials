{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LM with transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN8TTrk+xF1mfdiq98wGOjv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangxs131/pytorch_tutorials/blob/main/LM_with_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7shF8LuOmnj",
        "outputId": "70e37214-2389-42a1-db7a-38df2b8d8092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan  5 07:27:02 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P0    72W / 149W |   1167MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "本次使用pytorch中自带的Transformers的模型中Encoder部分，进行训练语言模型。\n",
        "来自于pytorch.org官网教程：https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "\n",
        "这里的预训练策略是判断下一个词，即词表中每个词出现在下一个位置的概率。\n",
        "使用的模型结构为transformer的Encoder层，然后加一个线性层最后log-softmax一下。\n",
        "\n",
        "输入为token embedding和position embedding"
      ],
      "metadata": {
        "id": "qgkqT9_NPTwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import Tuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder,TransformerEncoderLayer\n",
        "from torch.utils.data import dataset"
      ],
      "metadata": {
        "id": "4vMcS9ZRQtJS"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "下面使用transformer编写预训练模型"
      ],
      "metadata": {
        "id": "l_t-y8fLVlYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "  def __init__(self,ntoken,d_model,n_head,d_hid,nlayers,droput=0.5):\n",
        "    # ntoken 字典单词数；d_model 即输入特征维度，transformer为512，bert为768；n_head 多头注意力机制的头个数\n",
        "    # d_hid 为feedforward层中隐藏层的维度，一般为2048；n_layers 几层encoderlayer构成encoder\n",
        "    super().__init__()\n",
        "    self.model_type='Transformer'\n",
        "    self.d_model=d_model\n",
        "    self.pos_encoder=PositionalEmbedding(d_model,dropout)\n",
        "    encoder_layers=TransformerEncoderLayer(d_model,n_head,d_hid,dropout)\n",
        "    self.transformer_encoder=TransformerEncoder(encoder_layers,num_layers=nlayers)\n",
        "    self.encoder=nn.Embedding(ntoken,d_model)\n",
        "    self.decoder=nn.Linear(d_model,ntoken)\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  #初始化encoder和decoder的权重\n",
        "  def init_weights(self):\n",
        "    initrange=0.1\n",
        "    self.encoder.weight.data.uniform_(-initrange,initrange)\n",
        "    self.decoder.bias.data.zero_()\n",
        "    self.decoder.weight.data.uniform_(-initrange,initrange)\n",
        "  \n",
        "  def forward(self,src,src_mask):\n",
        "    # shape: src   [seq_len,batch_size]\n",
        "    #     src_mask [seq_len,seq_len]\n",
        "\n",
        "    #embedding\n",
        "    src=self.encoder(src)*math.sqrt(self.d_model)\n",
        "    src=self.pos_encoder(src)\n",
        "\n",
        "    output=self.transformer_encoder(src,src_mask)\n",
        "    output=self.decoder(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "f8NBhiYqRWjs"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "下面函数用于mask文本，即生成一个三角矩阵,矩阵左下部分包括对角线为0，而右上部分为-inf"
      ],
      "metadata": {
        "id": "eYOqygJwU6B9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "  return torch.triu(torch.ones(sz,sz)*float('-inf'),diagonal=1)\n",
        "generate_square_subsequent_mask(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw1sb8K-R5PH",
        "outputId": "4e541abc-5ef2-46d1-cdc1-2d7966480d24"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., -inf, -inf],\n",
              "        [0., 0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "下面编写PositionalEmbedding,这里的 positionalembedding 使用三角函数计算的固定位置embedding，维度大小与token embeddding一致，用于相加后得到输入到LM模型中的embedding。"
      ],
      "metadata": {
        "id": "dwJvQ2MQWc0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "  def __init__(self,d_model,dropout=0.1,max_len=5000):\n",
        "    super().__init__()\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    position=torch.arange(max_len).unsqueeze(1)\n",
        "    div_term=torch.exp(torch.arange(0,d_model,2)*(-math.log(10000.0)/d_model))\n",
        "    pe=torch.zeros(max_len,1,d_model)\n",
        "    pe[:,0,0::2]=torch.sin(position*div_term)\n",
        "    pe[:,0,1::2]=torch.cos(position*div_term)\n",
        "    self.register_buffer('pe',pe)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x+=self.pe[:x.size(0)]\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "id": "l2EDkpQ2XBtL"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load and batch data\n",
        "使用torchtext生成Wikitext-2 dataset，并通过batchify（）进行batch化"
      ],
      "metadata": {
        "id": "wX-yTKvx4SC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_iter=WikiText2(split='train')\n",
        "tokenizer=get_tokenizer('basic_english')\n",
        "vocab=build_vocab_from_iterator(map(tokenizer,train_iter),specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "def data_process(raw_text_iter):\n",
        "  data=[torch.tensor(vocab(tokenizer(item)),dtype=torch.long) for item in raw_text_iter]\n",
        "  return torch.cat(tuple(filter(lambda t:t.numel()>0,data)))\n",
        "\n",
        "train_iter,val_iter,test_iter=WikiText2()\n",
        "train_data=data_process(train_iter)\n",
        "val_data=data_process(val_iter)\n",
        "test_data=data_process(test_iter)\n",
        "\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data,batch_size):\n",
        "  #data shape [N]\n",
        "  # return [N//batch_size.batch_size]\n",
        "  seq_len=data.size(0)//batch_size\n",
        "  data=data[:seq_len*batch_size]\n",
        "  data=data.view(batch_size,seq_len).t().contiguous()\n",
        "  return data.to(device)\n",
        "\n",
        "batch_size=20\n",
        "eval_batch_size=10\n",
        "train_data=batchify(train_data,batch_size)\n",
        "val_data=batchify(val_data,eval_batch_size)\n",
        "test_data=batchify(test_data,eval_batch_size)"
      ],
      "metadata": {
        "id": "9da5ntq44p3b"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "下面get_batch把数据分为data和target,通过切块，如原数据2-5的token，则target为3-6的token.bptt为块最大长度。"
      ],
      "metadata": {
        "id": "3UhxKBgE9sMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bptt=35\n",
        "def get_batch(source,i)->Tuple[Tensor,Tensor]:\n",
        "  #source Tensor shape[full_seq_len,batch_size]\n",
        "  seq_len=min(bptt,len(source)-1-i)\n",
        "  data=source[i:i+seq_len]\n",
        "  target=source[i+1:i+1+seq_len].reshape(-1)\n",
        "  return data,target\n",
        "\n"
      ],
      "metadata": {
        "id": "GaNRdUSe6J46"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "初始化模型和参数定义"
      ],
      "metadata": {
        "id": "YQIZm1io-f7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ntoken=len(vocab)\n",
        "embedding_size=200\n",
        "d_hid=200\n",
        "nlayers=2\n",
        "n_head=2\n",
        "dropout=0.2\n",
        "model=TransformerModel(ntoken,embedding_size,n_head,d_hid,nlayers,dropout).to(device)"
      ],
      "metadata": {
        "id": "jYW2wkWR-H_s"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "运行模型，使用交叉熵作为损失函数，使用SGD优化器，设置初始化为5.0并使用StepLR进行调整学习率。在训练过程中使用nn.utils.clip_grad_norm_来防止梯度爆炸"
      ],
      "metadata": {
        "id": "mlaeQNvk-k0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import time\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "lr=5.0\n",
        "optimizer=torch.optim.SGD(model.parameters(),lr=lr)\n",
        "scheduler=torch.optim.lr_scheduler.StepLR(optimizer,1.0,gamma=0.95)\n",
        "\n",
        "def train(model):\n",
        "  model.train()\n",
        "  total_loss=0.\n",
        "  log_interval=200\n",
        "  start_time=time.time()\n",
        "  src_mask=generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "  num_batches=len(train_data)//bptt\n",
        "  for idx,i in enumerate(range(0,train_data.size(0)-1,bptt)):\n",
        "    data,targets=get_batch(train_data,i)\n",
        "    batch_size=data.size(0)\n",
        "    if batch_size!=bptt:\n",
        "      src_mask=src_mask[:batch_size,:batch_size]\n",
        "    output=model(data,src_mask)\n",
        "    loss=criterion(output.view(-1,ntoken),targets)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(),0.5)\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss+=loss.item()\n",
        "    if idx%log_interval ==0 and idx>0:\n",
        "      lr=scheduler.get_last_lr()[0]\n",
        "      ms_per_batch=(time.time()-start_time)*1000/log_interval\n",
        "      cur_loss=total_loss/log_interval\n",
        "      ppl=math.exp(cur_loss)\n",
        "      print('epoch {}|{}/{} batches|lr {} ms/batch {} | loss {} | ppl {} '.format(epoch,idx,num_batches,lr,ms_per_batch,cur_loss,ppl))\n",
        "      total_loss=0\n",
        "      start_time=time.time()\n",
        "\n",
        "def evaluate(model,eval_data):\n",
        "  model.eval()\n",
        "  total_loss=0.\n",
        "  src_mask=generate_square_subsequent_mask(bptt).to(device)\n",
        "  with torch.no_grad():\n",
        "    for i in range(0,eval_data.size(0)-1,bptt):\n",
        "      data,targets=get_batch(eval_data,i)\n",
        "      batch_size=data.size(0)\n",
        "      if batch_size!=bptt:\n",
        "        src_mask=src_mask[:batch_size,:batch_size]\n",
        "      output=model(data,src_mask)\n",
        "      output_flat=output.view(-1,ntoken)\n",
        "      total_loss+=batch_size*criterion(output_flat,targets).item()\n",
        "  return total_loss/(len(eval_data)-1)\n"
      ],
      "metadata": {
        "id": "l25vqSRT-8Zk"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "设置epoch开始训练，保存loss低的模型，调整学习率"
      ],
      "metadata": {
        "id": "JfUpwnxeEUdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss=float('inf')\n",
        "epochs=3\n",
        "best_model=None\n",
        "\n",
        "for epoch in range(1,epochs+1):\n",
        "  epoch_start_time=time.time()\n",
        "  train(model)\n",
        "  val_loss=evaluate(model,val_data)\n",
        "  val_ppl=math.exp(val_loss)\n",
        "  elapsed=time.time()-epoch_start_time\n",
        "  print('-'*88)\n",
        "  print('end of epoch {} |time: {}s | valid loss {},valid ppl {}'.format(epoch,elapsed,val_loss,val_ppl))\n",
        "  print('-'*88)\n",
        "\n",
        "  if val_loss<best_val_loss:\n",
        "    best_val_loss=val_loss\n",
        "    best_model=copy.deepcopy(model)\n",
        "\n",
        "  scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tidST5w6EezV",
        "outputId": "95a92e80-0f8b-426e-a110-fec8b945238f"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1|200/2928 batches|lr 5.0 ms/batch 37.98854351043701 | loss 8.242470648288727 | ppl 3798.9145048511164 \n",
            "epoch 1|400/2928 batches|lr 5.0 ms/batch 37.49191761016846 | loss 6.891486141681671 | ppl 983.8624903244288 \n",
            "epoch 1|600/2928 batches|lr 5.0 ms/batch 37.585289478302 | loss 6.443272621631622 | ppl 628.4601503418046 \n",
            "epoch 1|800/2928 batches|lr 5.0 ms/batch 37.57092475891113 | loss 6.3073224925994875 | ppl 548.5741692811933 \n",
            "epoch 1|1000/2928 batches|lr 5.0 ms/batch 37.6913857460022 | loss 6.19197496175766 | ppl 488.81053565999616 \n",
            "epoch 1|1200/2928 batches|lr 5.0 ms/batch 37.46937036514282 | loss 6.153678452968597 | ppl 470.444716648386 \n",
            "epoch 1|1400/2928 batches|lr 5.0 ms/batch 37.459876537323 | loss 6.114461686611175 | ppl 452.3524744243365 \n",
            "epoch 1|1600/2928 batches|lr 5.0 ms/batch 37.42375612258911 | loss 6.101904542446136 | ppl 446.70773427009357 \n",
            "epoch 1|1800/2928 batches|lr 5.0 ms/batch 37.43600130081177 | loss 6.013551225662232 | ppl 408.93295796960973 \n",
            "epoch 1|2000/2928 batches|lr 5.0 ms/batch 37.479562759399414 | loss 6.009534728527069 | ppl 407.29377400793743 \n",
            "epoch 1|2200/2928 batches|lr 5.0 ms/batch 37.384973764419556 | loss 5.891347165107727 | ppl 361.8924850573391 \n",
            "epoch 1|2400/2928 batches|lr 5.0 ms/batch 37.287434339523315 | loss 5.965168700218201 | ppl 389.61875129528204 \n",
            "epoch 1|2600/2928 batches|lr 5.0 ms/batch 37.4186635017395 | loss 5.944612114429474 | ppl 381.69128004030887 \n",
            "epoch 1|2800/2928 batches|lr 5.0 ms/batch 37.380725145339966 | loss 5.883966178894043 | ppl 359.2311951682996 \n",
            "----------------------------------------------------------------------------------------\n",
            "end of epoch 1 |time: 113.50342869758606s | valid loss 5.783409386539637,valid ppl 324.8648944583217\n",
            "----------------------------------------------------------------------------------------\n",
            "epoch 2|200/2928 batches|lr 4.75 ms/batch 37.57812023162842 | loss 5.864493680000305 | ppl 352.30373251606414 \n",
            "epoch 2|400/2928 batches|lr 4.75 ms/batch 37.34256982803345 | loss 5.859110879898071 | ppl 350.41244672193415 \n",
            "epoch 2|600/2928 batches|lr 4.75 ms/batch 37.41596341133118 | loss 5.6643833780288695 | ppl 288.41008627942375 \n",
            "epoch 2|800/2928 batches|lr 4.75 ms/batch 37.3477566242218 | loss 5.697970914840698 | ppl 298.2615883896675 \n",
            "epoch 2|1000/2928 batches|lr 4.75 ms/batch 37.373738288879395 | loss 5.647077343463898 | ppl 283.46179252879546 \n",
            "epoch 2|1200/2928 batches|lr 4.75 ms/batch 37.45342254638672 | loss 5.674679293632507 | ppl 291.3948713676591 \n",
            "epoch 2|1400/2928 batches|lr 4.75 ms/batch 37.4070930480957 | loss 5.677956376075745 | ppl 292.3513627792722 \n",
            "epoch 2|1600/2928 batches|lr 4.75 ms/batch 37.3935604095459 | loss 5.708370542526245 | ppl 301.37958275059975 \n",
            "epoch 2|1800/2928 batches|lr 4.75 ms/batch 37.33510971069336 | loss 5.650016288757325 | ppl 284.29609661633197 \n",
            "epoch 2|2000/2928 batches|lr 4.75 ms/batch 37.251689434051514 | loss 5.664531495571136 | ppl 288.4528080364111 \n",
            "epoch 2|2200/2928 batches|lr 4.75 ms/batch 37.328031063079834 | loss 5.548918380737304 | ppl 256.95947322712004 \n",
            "epoch 2|2400/2928 batches|lr 4.75 ms/batch 37.45877981185913 | loss 5.647197201251983 | ppl 283.4957696684265 \n",
            "epoch 2|2600/2928 batches|lr 4.75 ms/batch 37.4368941783905 | loss 5.643674211502075 | ppl 282.49877421131066 \n",
            "epoch 2|2800/2928 batches|lr 4.75 ms/batch 37.37836003303528 | loss 5.576843340396881 | ppl 264.2361842207742 \n",
            "----------------------------------------------------------------------------------------\n",
            "end of epoch 2 |time: 113.19045758247375s | valid loss 5.611622434276254,valid ppl 273.58775627118655\n",
            "----------------------------------------------------------------------------------------\n",
            "epoch 3|200/2928 batches|lr 4.5125 ms/batch 37.62157678604126 | loss 5.605816724300385 | ppl 272.00398700641415 \n",
            "epoch 3|400/2928 batches|lr 4.5125 ms/batch 37.450417280197144 | loss 5.629462370872497 | ppl 278.5123410276094 \n",
            "epoch 3|600/2928 batches|lr 4.5125 ms/batch 37.42631554603577 | loss 5.414911942481995 | ppl 224.73275539863215 \n",
            "epoch 3|800/2928 batches|lr 4.5125 ms/batch 37.375614643096924 | loss 5.478393573760986 | ppl 239.46172063937598 \n",
            "epoch 3|1000/2928 batches|lr 4.5125 ms/batch 37.3097288608551 | loss 5.430163152217865 | ppl 228.18647151607638 \n",
            "epoch 3|1200/2928 batches|lr 4.5125 ms/batch 37.364972829818726 | loss 5.465131275653839 | ppl 236.30687441222165 \n",
            "epoch 3|1400/2928 batches|lr 4.5125 ms/batch 37.40870118141174 | loss 5.4910869836807255 | ppl 242.52067965772181 \n",
            "epoch 3|1600/2928 batches|lr 4.5125 ms/batch 37.43084192276001 | loss 5.51389618396759 | ppl 248.1159516730332 \n",
            "epoch 3|1800/2928 batches|lr 4.5125 ms/batch 37.277668714523315 | loss 5.4568069815635685 | ppl 234.34795113257033 \n",
            "epoch 3|2000/2928 batches|lr 4.5125 ms/batch 37.46373176574707 | loss 5.4800563192367555 | ppl 239.8602157381397 \n",
            "epoch 3|2200/2928 batches|lr 4.5125 ms/batch 37.381653785705566 | loss 5.35404022693634 | ppl 211.46092442731486 \n",
            "epoch 3|2400/2928 batches|lr 4.5125 ms/batch 37.43680000305176 | loss 5.469163734912872 | ppl 237.261696101859 \n",
            "epoch 3|2600/2928 batches|lr 4.5125 ms/batch 37.352964878082275 | loss 5.466945788860321 | ppl 236.73604560727344 \n",
            "epoch 3|2800/2928 batches|lr 4.5125 ms/batch 37.327563762664795 | loss 5.401040005683899 | ppl 221.63679991490994 \n",
            "----------------------------------------------------------------------------------------\n",
            "end of epoch 3 |time: 113.23414993286133s | valid loss 5.591669035714064,valid ppl 268.18285314534063\n",
            "----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "测试集结果"
      ],
      "metadata": {
        "id": "aeeVI3eEFq6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss=evaluate(best_model,test_data)\n",
        "test_ppl=math.exp(test_loss)\n",
        "print('-'*88)\n",
        "print('end of epoch {} | test loss {},valid ppl {}'.format(epoch,test_loss,test_ppl))\n",
        "print('-'*88)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnBAzxv8FsYF",
        "outputId": "9694536d-32ea-4b1e-8f4b-e21fa6259574"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------\n",
            "end of epoch 3 | test loss 5.502080969667734,valid ppl 245.201658932124\n",
            "----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}